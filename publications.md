---
layout: page
permalink: /publications/index.html
title: Publications
---

# Publications

Lastest Update: 2025.03.12
## First author
<sup>*</sup> indicates equal contribution.
<table style="width: 100%; table-layout: fixed;">
  <tr>
    <!-- 左栏：放置图像或GIF -->
    <td style="width: 35%;">
      <img src="https://yyvhang.github.io/images/SIGMAN.gif" width="90%" height="100%"/>
    </td>
    <td style="width: 55%;">
      SIGMAN: Scaling 3D Human Gaussian Generation with Millions of Assets. (arxiv2025) <br><a href="https://yyvhang.github.io/SIGMAN_3D/"><u>Project</u></a>, <a href="https://arxiv.org/abs/2504.06982"><u>Paper</u></a>, <a href="https://github.com/yyvhang/SIGMAN_release"><u>Github</u></a> <br>
      <strong>Yuhang Yang<sup>*</sup></strong>, Fengqi Liu<sup>*</sup>, Yixing Lu, Qin Zhao, Pingyu Wu, Wei Zhai, Ran Yi, Yang Cao, Lizhuang Ma, Zheng-Jun Zha, Junting Dong.
    </td>
  </tr>
</table>
<br>
<table style="width: 100%; table-layout: fixed;">
  <tr>
    <!-- 左栏：放置图像或GIF -->
    <td style="width: 35%;">
      <img src="https://yyvhang.github.io/images/VideoGen_Eval.png" width="90%" height="100%"/>
    </td>
    <td style="width: 55%;">
      VideoGen-Eval: Agent-based System for Video Generation Evaluation. (arxiv2025) <br><a href="https://ailab-cvc.github.io/VideoGen-Eval/"><u>Project</u></a>, <a href="https://arxiv.org/abs/2503.23452"><u>Evaluation Paper</u></a>, <a href="https://arxiv.org/pdf/2410.05227"><u>Survey Paper</u></a>, <a href="https://github.com/AILab-CVC/VideoGen-Eval"><u>Github</u></a> <br>
      <strong>Yuhang Yang</strong>, Ke Fan, Shangkun Sun, Hongxiang Li, Ailing Zeng, Feilin Han, Wei Zhai, Wei Liu, Yang Cao, Zheng-Jun Zha.
    </td>
  </tr>
</table>
<br>
<table style="width: 100%; table-layout: fixed;">
  <tr>
    <!-- 左栏：放置图像或GIF -->
    <td style="width: 35%;">
      <img src="https://yyvhang.github.io/images/EgoChoir.gif" width="90%" height="100%"/>
    </td>
    <td style="width: 55%;">
      EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views. <font color='red'>(NeurIPS-2024)</font> <br><a href="https://yyvhang.github.io/EgoChoir/"><u>Project</u></a>, <a href="https://arxiv.org/abs/2405.13659"><u>Paper</u></a>, <a href="https://github.com/yyvhang/EgoChoir_release?tab=readme-ov-file"><u>Code</u></a> <br>
      <strong>Yuhang Yang</strong>, Wei Zhai, Chengfeng Wang, Chengjun Yu,Yang Cao, Zheng-Jun Zha.
    </td>
  </tr>
</table>
<br>
<table style="width: 100%; table-layout: fixed;">
  <tr>
    <!-- 左栏：放置图像或GIF -->
    <td style="width: 35%;">
      <img src="https://yyvhang.github.io/images/LEMON.gif" width="90%" height="100%"/>
    </td>
    <td style="width: 55%;">
      LEMON: Learning 3D Human-Object Interaction Relation from 2D Images. <font color='red'>(CVPR-2024)</font> <br><a href="https://yyvhang.github.io/LEMON/"><u>Project</u></a>, <a href="https://arxiv.org/abs/2312.08963"><u>Paper</u></a>, <a href="https://github.com/yyvhang/lemon_3d"><u>Code</u></a>, <a href="https://www.bilibili.com/video/BV19H4y1c729/?spm_id_from=333.337.search-card.all.click&vd_source=a1202e8b3f4113eed71d8688aa647975"><u>bilibili</u></a> <br>
      <strong>Yuhang Yang</strong>, Wei Zhai, Hongchen Luo, Yang Cao, Zheng-Jun Zha.
    </td>
  </tr>
</table>
<br>
<table style="width: 100%; table-layout: fixed;">
  <tr>
    <!-- 左栏：放置图像或GIF -->
    <td style="width: 35%;">
      <img src="https://yyvhang.github.io/images/IAG.gif" width="90%" height="100%"/>
    </td>
    <td style="width: 55%;">
      Grounding 3D Object Affordance from 2D Interactions in Images. <font color='red'>(ICCV-2023)</font> <br><a href="https://yyvhang.github.io/publications/IAG/index.html"><u>Project</u></a>, <a href="https://arxiv.org/abs/2303.10437"><u>Paper</u></a>, <a href="https://github.com/yyvhang/IAGNet"><u>Code</u></a> <br>
      <strong>Yuhang Yang</strong>, Wei Zhai, Hongchen Luo, Yang Cao, Jiebo Luo, Zheng-Jun Zha.
    </td>
  </tr>
</table>

## Co-author
<table style="width: 100%; table-layout: fixed;">
  <tr>
    <!-- 左栏：放置图像或GIF -->
    <td style="width: 35%;">
      <img src="https://yyvhang.github.io/images/GREAT.png" width="90%" height="100%"/>
    </td>
    <td style="width: 55%;">
      GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding. <font color='red'>(CVPR-2025)</font> <br><a href="https://yawen-shao.github.io/GREAT/"><u>Project</u></a>, <a href="https://arxiv.org/abs/2411.19626"><u>Paper</u></a>, <a href="https://github.com/yawen-shao/GREAT_code"><u>Code</u></a> <br>
      Yawen Shao, Wei Zhai, <strong>Yuhang Yang</strong>, Hongchen Luo, Yang Cao, Zheng-Jun Zha.
    </td>
  </tr>
</table>

<table style="width: 100%; table-layout: fixed;">
  <tr>
    <td style="width: 35%; text-align: center; vertical-align: middle;">
      <img src="https://yyvhang.github.io/images/Dispose.gif" width="70%" height="80%"/>
    </td>
    <td style="width: 55%;">
      DisPose: Disentangling Pose Guidance for Controllable Human Image Animation. <font color='red'>(ICLR-2025)</font> <br><a href="https://lihxxx.github.io/DisPose/"><u>Project</u></a>, <a href="https://arxiv.org/abs/2412.09349"><u>Paper</u></a>, <a href="https://github.com/lihxxx/DisPose"><u>Github</u></a> <br>
      Hongxiang Li, Yaowei Li, <strong>Yuhang Yang</strong>, Junjie Cao, Zhihong Zhu, Xuxin Chen, Long Chen.
    </td>
  </tr>
</table>
<br>
<table style="width: 100%; table-layout: fixed;">